# -*- coding: utf-8 -*-
"""Tyler's Copy of 97%Acc_Gopher_Bin CNN Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mEkOuMxi6jED9NsIe3cTtMLlnITUmqAW
"""

!pip install keras-tuner
!pip install h5py

# Python program to create
# Image Classifier using CNN

# Importing the required libraries
import cv2
import os
import numpy as np
from random import shuffle
from tqdm import tqdm
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras 
from keras_tuner import RandomSearch
from keras_tuner.engine.hyperparameters import HyperParameters
from keras.models import model_from_json

!unzip Waste.zip
!unzip garbage_classification_\(1\).zip

TRAIN_DIR = 'Waste' 
TEST_DIR = "garbage_classification"
IMG_SIZE = 50
LR = 1e-5


MODEL_NAME = 'bio_vs_nonbio-{}-{}.model'.format(LR, '6conv-basic')

'''Labelling the dataset'''
def label_img(img):
	word_label = img.split('(')[0]
	# print(word_label)
	if word_label in ['org ','paper ','text ','wood ']: return 1
	else: return 0

'''Creating the training data'''
def create_train_data():
	# Creating an empty list where we should store the training data
	# after a little preprocessing of the data
	training_data = []

	for img in os.listdir(TRAIN_DIR):

		# labeling the images
		label = label_img(img)
		# print(label)

		path = os.path.join(TRAIN_DIR, img)

		# loading the image from the path and then converting them into
		# grayscale for easier covnet prob
		img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)

		# resizing the image for processing them in the covnet
		try:
				img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
		except:
			break
		
		img = np.array(img)
		img = np.expand_dims(img, axis=-1)
		# final step-forming the training data list with numpy array of the images
		training_data.append([img, np.array(label)])

	# shuffling of the training data to preserve the random state of our data
	shuffle(training_data)
 
	# print(training_data[0][0].shape)
	# saving our trained data for further uses if required
	np.save('train_data.npy', training_data)
	return training_data

'''Processing the given test data'''
def process_test_data():
	testing_data = []
	for img in tqdm(os.listdir(TEST_DIR)):
		path = os.path.join(TEST_DIR, img)
		img_num = img.split('. ')[0]
		# if isinstance(img_num, str):
		# 	print("img_num ", img_num)
		img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
	  
		try: 
			img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
		except:
			break

		img = np.array(img)
		img = np.expand_dims(img, axis=-1)

		testing_data.append([img, np.array(img_num)])
		
	shuffle(testing_data)
	np.save('test_data.npy', testing_data)
	return testing_data

'''Running the training and the testing in the dataset for our model'''
train_data = create_train_data()
test_data = process_test_data()

# print("train_data.shape ", train_data)

'''CNN'''
def build_model(hp):
	model = keras.Sequential([
    keras.layers.Conv2D(
        filters=hp.Int('conv_1_filter', min_value=64, max_value=128, step=16),
        kernel_size=hp.Choice('conv_1_kernel', values = [2,5]),
        activation='relu',
        input_shape=(50,50,1)
    ),
    keras.layers.Conv2D(
        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),
        kernel_size=hp.Choice('conv_2_kernel', values = [2,5]),
        activation='relu'
    ),
    keras.layers.Flatten(),
    keras.layers.Dense(
        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),
        activation='relu'
    ),
    keras.layers.Dense(2, activation='softmax')
  ])
  
	model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-5])),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
	return model


'''Setting up the features and labels'''
# X-Features & Y-Labels
x =[]
y =[]

test_x = []
test_y = []


for datasets, labels in iter(train_data):
	x.append(datasets)
	y.append(labels)

X = np.stack(x)
Y = np.stack(y)


for img, num in iter(test_data):

	 test_x.append(img)
	 test_y.append(num)
	

Test_x = np.stack(test_x)
Test_y = np.stack(test_y)

tuner_search=RandomSearch(build_model,
                          objective='val_accuracy',
                          max_trials=300, directory='output',project_name="Garbage Classification")



'''Fitting the data into our model'''

tuner_search.search(X, Y, epochs=10, validation_split=0.1)
model=tuner_search.get_best_models(num_models=1)[0]
model.summary()
history = model.fit(X, Y, epochs=25, validation_split=0.1, initial_epoch=0)

model.save(MODEL_NAME)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

# tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)
# tflite_model = tf_lite_converter.convert()
# tflite_model_name = "TFLITENEW.tflite"
# open(tflite_model_name, "wb").write(tflite_model)

from google.colab import drive
drive.mount('/content/drive')